{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7001ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572014c1",
   "metadata": {},
   "source": [
    "# パスの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "# CSVファイルが格納されているフォルダのパスを指定\n",
    "folder_path = Path(\"folder_path_here\")  # 実際のフォルダパスを指定してください\n",
    "\n",
    "# フォルダ内のCSVファイルを取得\n",
    "csv_files = [file for file in folder_path.glob(\"*.csv\")]\n",
    "\n",
    "# 各CSVファイルを読み込む\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        with open(csv_file, newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                # 各行のデータを処理する\n",
    "                print(row)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {csv_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ecfb6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m BASE_DIR \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/機械学習/sake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m DATA_DIR \u001b[38;5;241m=\u001b[39m BASE_DIR\n\u001b[0;32m      3\u001b[0m FEATURES_DIR \u001b[38;5;241m=\u001b[39m BASE_DIR\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path(\"/content/drive/MyDrive/機械学習/sake\")\n",
    "DATA_DIR = BASE_DIR\n",
    "FEATURES_DIR = BASE_DIR.joinpath(\"features\")\n",
    "cite_filepath = DATA_DIR.joinpath(\"cite.csv\")\n",
    "train_filepath = DATA_DIR.joinpath(\"train.csv\")\n",
    "test_filepath = DATA_DIR.joinpath(\"test.csv\")\n",
    "sub_filepath = DATA_DIR.joinpath(\"sample_submission.csv\")\n",
    "df_cite = pd.read_csv(cite_filepath)\n",
    "df_train = pd.read_csv(train_filepath)\n",
    "df_test = pd.read_csv(test_filepath)\n",
    "df_sub = pd.read_csv(sub_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3db4b2b",
   "metadata": {},
   "source": [
    "## パスの利用（dataframe挿入)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15f510f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_cite' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cite_filenames \u001b[38;5;241m=\u001b[39m \u001b[43mdf_cite\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcite_filename\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[0;32m      2\u001b[0m df_cite[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(CITE_IMG_DIR\u001b[38;5;241m.\u001b[39mjoinpath(filename)) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m cite_filenames]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_cite' is not defined"
     ]
    }
   ],
   "source": [
    "cite_filenames = df_cite[\"cite_filename\"].to_list()\n",
    "df_cite[\"path\"] = [str(CITE_IMG_DIR.joinpath(filename)) for filename in cite_filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb341a2f",
   "metadata": {},
   "source": [
    "#　ファイル読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9060424",
   "metadata": {},
   "source": [
    "## 一つのファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46915b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7463bae3",
   "metadata": {},
   "source": [
    "## 1つのファイルに複数フォルダ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb49099",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in glob.glob(\"train/*.csv*\"):\n",
    "    tmp_df=pd.read_csv(i)\n",
    "    df = pd.concat([df, tmp_df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee406a8d",
   "metadata": {},
   "source": [
    "# 欠損値処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363ac45",
   "metadata": {},
   "source": [
    "## 行操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b5ebf",
   "metadata": {},
   "source": [
    "### 全て欠損値の行を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b42777b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='all', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b122ce1",
   "metadata": {},
   "source": [
    "### 欠損値の1つでもある行を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7319cd5",
   "metadata": {},
   "source": [
    "## 列操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b3742",
   "metadata": {},
   "source": [
    "### 全て欠損値の列を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45272959",
   "metadata": {},
   "source": [
    "# 単一のユニーク数の列を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c6a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, 3, 4],\n",
    "                   'B': [1, 1, 1, 1],\n",
    "                   'C': [1, 2, 3, 4]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f930ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, df.nunique() != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fe41e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of    A  C\n",
       "0  1  1\n",
       "1  2  2\n",
       "2  3  3\n",
       "3  4  4>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08147df6",
   "metadata": {},
   "source": [
    "# いくつかの文字を抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['列名'] = df['列名'].apply(lambda x: int(str(x)[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f9792",
   "metadata": {},
   "source": [
    "# 複数列のgroupby(複数列ではxfeat使えない)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c2d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_cols = ['mean_temp', 'max_temp', 'min_temp', 'sum_rain', 'sun_time', 'mean_humid']\n",
    "gb_df = wea_df.groupby(['area', 'year', 'month'])[agg_cols].agg(['mean','max','min']).reset_index()\n",
    "\n",
    "new_cols = []\n",
    "for col1, col2 in gb_df.columns:\n",
    "    if col2:\n",
    "        new_cols.append(col2+'_'+col1)\n",
    "    else:\n",
    "        new_cols.append(col1)\n",
    "\n",
    "gb_df.columns = new_cols\n",
    "gb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233128a",
   "metadata": {},
   "source": [
    "# sklearnエンコーディング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a9a438",
   "metadata": {},
   "source": [
    "## エンコーディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a250aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoded_categories = encoder.fit_transform(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = pd.get_dummies(df, columns=['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352bab02",
   "metadata": {},
   "source": [
    "# xfeatの操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dcfd7b",
   "metadata": {},
   "source": [
    "## aggrecation操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edit=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d39a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dfs = []\n",
    "\n",
    "def get_agg_df(df, group_col):\n",
    "\n",
    "    agg_df, agg_cols = aggregation(df,\n",
    "                        group_key=group_col,\n",
    "                        group_values=['',''],\n",
    "                        agg_methods=['count', 'mean', 'min', 'max'],\n",
    "                        )\n",
    "\n",
    "    return agg_df[agg_cols]\n",
    "\n",
    "group_col = '市区町村名'\n",
    "agg_dfs.append(get_agg_df(df_edit1, group_col))\n",
    "agg_dfs[0].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edit2=pd.concat([df_edit1]+agg_dfs,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8661411d",
   "metadata": {},
   "source": [
    "## combi操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a786bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edit=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_combi = ConcatCombination(drop_origin=True, r=2)\n",
    "concat_df=concat_combi.fit_transform(df_edit1[['都道府県名', '最寄駅：距離（分）','間取り']].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f66ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edit2=pd.concat([df_edit1,concat_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717b159a",
   "metadata": {},
   "source": [
    "## ArithmeticCombinationsの適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9970a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edit=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca04757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ArithmeticCombinationsの適用\n",
    "arithmetic_combinations = ArithmeticCombinations(\n",
    "    operator='*',\n",
    "    drop_origin=True,  \n",
    "    # 元の特徴量を削除するかどうか\n",
    "\n",
    "    # 使用する算術演算子\n",
    "    r=2,\n",
    "    # 算術演算子の数（ここでは2つの特徴量を組み合わせる）\n",
    ")\n",
    "output_df = arithmetic_combinations.fit_transform(df_edit1[['市区町村名_te', '面積（㎡）']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edit2=pd.concat([df_edit1,concat_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ef6db",
   "metadata": {},
   "source": [
    "## SelectNumerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbf083",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Pipeline(\n",
    "    [\n",
    "        SelectNumerical(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "num_comb_df = encoder.fit_transform(df_edit2)\n",
    "num_comb_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7930a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    SelectCategorical(),  # カテゴリカルデータの選択\n",
    "    LabelEncoder(output_suffix=''),  # ラベルエンコーディング\n",
    "    LambdaEncoder(encoder=lambda x: pd.get_dummies(x, prefix='color')),  # ワンホットエンコーディング\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d268a1f",
   "metadata": {},
   "source": [
    "## SelectCategorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a2b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Pipeline([\n",
    "    SelectCategorical(),\n",
    "    LabelEncoder(output_suffix=\"\"),\n",
    "])\n",
    "le_df = encoder.fit_transform(df_edit2)\n",
    "le_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f18475",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy=pd.concat([num_comb_df,le_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d6192",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2d94af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m      6\u001b[0m xgb_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:linear\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m123\u001b[39m\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 15\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train\u001b[49m)\n\u001b[0;32m     16\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     17\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_train)  \u001b[38;5;66;03m# Convert y_train to numpy array\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'reg:linear',\n",
    "    'learning_rate': 0.1,\n",
    "    'reg_lambda': 1,\n",
    "    'max_depth': 8,\n",
    "    'seed': 123\n",
    "}\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_train = np.array(y_train)  # Convert y_train to numpy array\n",
    "y_test = np.array(y_test)  # Convert y_test to numpy array\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_scaled, label=y_train,feature_names=X_train.columns)\n",
    "dtest = xgb.DMatrix(X_test_scaled, label=y_test,feature_names=X_train.columns)  # Provide labels for evaluation\n",
    "\n",
    "evals = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "\n",
    "model = xgb.train(xgb_params, dtrain, num_boost_round=400,\n",
    "                  early_stopping_rounds=20, evals=evals, verbose_eval=True)\n",
    "\n",
    "y_pred_train = model.predict(dtrain)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "print(\"Train RMSE:\", rmse_train)\n",
    "\n",
    "y_pred_test = model.predict(dtest)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "print(\"Test RMSE:\", rmse_test)\n",
    "print(\"Test MAE:\", mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9664b",
   "metadata": {},
   "source": [
    "# lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'num_leaves': 42,\n",
    "    'max_depth': 7,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    'subsample_freq': 1,\n",
    "    \"bagging_fraction\": 0.95,\n",
    "    'min_data_in_leaf': 2,\n",
    "    'learning_rate': 0.1,\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"lambda_l1\": 0.1,\n",
    "    \"lambda_l2\": 10,\n",
    "    \"verbosity\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"num_boost_round\": 50000,\n",
    "    \"early_stopping_rounds\": 100\n",
    "}\n",
    "\n",
    "train_data = lgb.Dataset(train_x, label=train_y)\n",
    "val_data = lgb.Dataset(val_x, label=val_y)\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data, \n",
    "    categorical_feature = cat_cols,\n",
    "    valid_names = ['train', 'valid'],\n",
    "    valid_sets =[train_data, val_data], \n",
    "    verbose_eval = 100,\n",
    ")\n",
    "\n",
    "val_pred = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "score = mean_absolute_error(val_y, val_pred)\n",
    "\n",
    "pred_df = pd.DataFrame(sorted(zip(val_x.index, val_pred, val_y)), columns=['index', 'predict', 'actual'])\n",
    "\n",
    "feature_imp = pd.DataFrame(sorted(zip(model.feature_importance(), train_x.columns)), columns=['importance', 'feature'])\n",
    "\n",
    "print(f'score: {score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea798b1",
   "metadata": {},
   "source": [
    "# トレーニングデータの絞り込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9190f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds = test_df['kind'].unique()\n",
    "train_df = train_df[train_df['kind'].isin(kinds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db2ef64",
   "metadata": {},
   "source": [
    "# googlecolab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52888287",
   "metadata": {},
   "source": [
    "## 準備　googledriveマウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebddd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "HOME = Path(\"/content/drive/MyDrive/competition/nishika_sake\")\n",
    "NOTEBOOK_NAME = requests.get(\"http://172.28.0.12:9000/api/sessions\").json()[0][\"name\"][:-6]\n",
    "EXP_NAME = Config.name if Config.name is not None else NOTEBOOK_NAME\n",
    "INPUTS = HOME / \"inputs\"  # input data\n",
    "OUTPUTS = HOME / \"outputs\"\n",
    "INTERMIDIATES = HOME / \"intermidiates\"  # intermidiate outputs\n",
    "SUBMISSIONS = HOME / \"submissions\"\n",
    "OUTPUTS_EXP = OUTPUTS / EXP_NAME\n",
    "EXP_MODELS = OUTPUTS_EXP / \"models\"\n",
    "EXP_REPORTS = OUTPUTS_EXP / \"reports\"\n",
    "EXP_PREDS = OUTPUTS_EXP / \"predictions\"\n",
    "SCRIPTS = HOME / \"scripts\"\n",
    "\n",
    "CITE_IMAGES = Path(\"cite_images\")\n",
    "QUERY_IMAGES = Path(\"query_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(Config):\n",
    "    # mount\n",
    "    from google.colab import drive\n",
    "    CONTENT_DRIVE = Path(\"/content/drive\")\n",
    "    if not CONTENT_DRIVE.is_dir():\n",
    "        drive.mount(CONTENT_DRIVE.as_posix())\n",
    "        # for d in [...]: d.mkdir(parents=True, exist_ok=True): ループを使って、事前に定義されたディレクトリパスを順に処理しています。mkdir 関数を使用して、ディレクトリを作成しています。\n",
    "# parents=True は親ディレクトリも存在しなければ作成することを示し、exist_ok=True は既にディレクトリが存在している場合にエラーを出さないようにします。\n",
    "    for d in [\n",
    "        HOME,\n",
    "        INPUTS,\n",
    "        SUBMISSIONS,\n",
    "        EXP_MODELS,\n",
    "        EXP_REPORTS,\n",
    "        EXP_PREDS,\n",
    "        INTERMIDIATES,\n",
    "        SCRIPTS,\n",
    "    ]:\n",
    "        d.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputのとこにログファイル\n",
    "LOGGER = Logger(OUTPUTS_EXP.as_posix())\n",
    "wandb.login()  # need wandb account\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5c2a7e",
   "metadata": {},
   "source": [
    "## zip開封"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content 直下で解凍 (ターミナルで実行が良さそう)\n",
    "if not QUERY_IMAGES.is_dir():\n",
    "    #Windowsのファイルパスを POSIX スタイル（スラッシュ / 区切り）に変換するために使われることが多いです。\n",
    "    query_images = (INPUTS / \"query_images.zip\").as_posix()\n",
    "    #cpファイルのコピー\n",
    "    ! cp $query_images .\n",
    "    # ! cp /content/drive/MyDrive/competition/nishika_sake/inputs/query_images.zip .\n",
    "    ! unzip ./query_images.zip\n",
    "\n",
    "if not CITE_IMAGES.is_dir():\n",
    "    cite_images = (INPUTS / \"cite_images.zip\").as_posix()\n",
    "    ! cp $cite_images .\n",
    "    # ! cp /content/drive/MyDrive/competition/nishika_sake/inputs/cite_images.zip .\n",
    "    ! unzip ./cite_images.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ec07d",
   "metadata": {},
   "source": [
    "## データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64559876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data\n",
    "train_df = pd.read_csv(INPUTS / \"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a81ad1",
   "metadata": {},
   "source": [
    "## ラベル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc4c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_label(train_df) -> pd.DataFrame:\n",
    "    le = LabelEncoder()\n",
    "    train_df[\"brand_id_label\"] = le.fit_transform(train_df[\"brand_id\"])\n",
    "    train_df[\"meigara_label\"] = le.fit_transform(train_df[\"meigara\"])\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c56a36",
   "metadata": {},
   "source": [
    "## データフレームのパス列の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8644dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filepath(input_df):\n",
    "    def join_path(dirpath, filename):\n",
    "        return (dirpath / filename).as_posix()\n",
    "#assign データフレームに新しい列を追加\n",
    "#lambda x: ...: ラムダ関数（無名関数）を定義しています。x は filename 列の各要素を示します。\n",
    "# このラムダ関数は、条件に基づいてファイルパスを生成するために使用されます。 \"2\" の場合は QUERY_IMAGES\n",
    "    output_df = input_df.assign(\n",
    "        filepath=input_df[\"filename\"].apply(\n",
    "            lambda x: join_path(QUERY_IMAGES, x) if str(x)[0] == \"2\" else join_path(CITE_IMAGES, x)\n",
    "        )\n",
    "    )\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdfe2b2",
   "metadata": {},
   "source": [
    "# 辞書の作成、利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824bd0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームから辞書の作成 \n",
    "MLABEL2MEIGARA = train_df[[\"meigara\", \"meigara_label\"]].set_index(\"meigara_label\").to_dict()[\"meigara\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584ff69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b9b6ef1",
   "metadata": {},
   "source": [
    "# cvsplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a12c6",
   "metadata": {},
   "source": [
    "## pytorch用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fold_idx(config, train_df):\n",
    "    fold = StratifiedKFold(n_splits=config.n_fold, shuffle=True, random_state=config.seed)\n",
    "#k分割してトレーニングセットと検証セット分割する\n",
    "    train_df[\"fold\"] = -1\n",
    "    for i_fold, (train_index, val_index) in enumerate(\n",
    "        fold.split(train_df, train_df[config.target_columns])\n",
    "    ):\n",
    "    #fold値を代入\n",
    "        train_df.iloc[val_index, train_df.columns.get_loc(\"fold\")] = int(i_fold)\n",
    "    train_df[\"fold\"] = train_df[\"fold\"].astype(int)\n",
    "    return train_df\n",
    "# train_df = add_fold_idx(config=Config, train_df=train_df)\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75071d0b",
   "metadata": {},
   "source": [
    "# pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e399f4",
   "metadata": {},
   "source": [
    "## データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    competition = \"sake\"\n",
    "    name = \"meigara_classification_baseline\"\n",
    "\n",
    "    debug = False\n",
    "\n",
    "    training = True\n",
    "    evaluation = True\n",
    "    embedding = True\n",
    "\n",
    "    seed = 8823\n",
    "    n_fold = 5\n",
    "    trn_fold = [0,1,2,3,4]\n",
    "\n",
    "    target_columns = [\"meigara_label\"]\n",
    "    size = 512\n",
    "\n",
    "    model_name = \"tf_efficientnet_b0_ns\"\n",
    "    max_epochs = 5\n",
    "    train_batch_size = 32\n",
    "    valid_batch_size = 128\n",
    "    num_workers = 4\n",
    "    gradient_accumulation_steps = 1\n",
    "    clip_grad_norm = 1000\n",
    "\n",
    "    optimizer = dict(\n",
    "        optimizer_name=\"AdamW\",\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-2,\n",
    "        eps=1e-6,\n",
    "        beta=(0.9, 0.999),\n",
    "        encoder_lr=1e-4,\n",
    "        decoder_lr=1e-4,\n",
    "    )\n",
    "\n",
    "    scheduler = dict(\n",
    "        scheduler_name=\"cosine\",\n",
    "        num_warmup_steps_rate=0,\n",
    "        num_cycles=0.5,\n",
    "    )\n",
    "    batch_scheduler = True\n",
    "\n",
    "\n",
    "if Config.debug:\n",
    "    Config.max_epochs = 2\n",
    "    Config.n_fold = 2\n",
    "    Config.trn_fold = [0, 1]\n",
    "    Config.name = Config.name + \"_debug\"\n",
    "    Config.size = 128\n",
    "    Config.train_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "#Noneにすることで後で指定できる\n",
    "#__len__メソッドと__getitem__は必要\n",
    "    def __init__(self,df, target_columns=Config.target_columns, transform_fn=None):\n",
    "        self.df = df\n",
    "        self.file_names = df[\"filepath\"].to_numpy()\n",
    "        self.targets = df[target_columns].to_numpy()\n",
    "        if len(target_columns) == 1:\n",
    "            self.targets = np.ravel(self.targets)\n",
    "\n",
    "        self.transform_fn = transform_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath = self.file_names[idx]\n",
    "        image = cv2.imread(filepath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform_fn:\n",
    "            image = self.transform_fn(image=image)[\"image\"]\n",
    "        target = torch.tensor(self.targets[idx])\n",
    "        return {\"images\":image, \"targets\":target}\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform_fn=None):\n",
    "        self.df = df\n",
    "        self.file_names = df[\"filepath\"].to_numpy()\n",
    "        self.transform_fn = transform_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath = self.file_names[idx]\n",
    "        image = cv2.imread(filepath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform_fn:\n",
    "            image = self.transform_fn(image=image)[\"image\"]\n",
    "        return {\"images\":image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c068153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(*, size, data=\"train\"):\n",
    "\n",
    "    if data == 'train':\n",
    "        return A.Compose([\n",
    "            # 正方形切り出し　バイリニア補間、バイキュービック補間\n",
    "            A.RandomResizedCrop(size, size, scale=(0.85, 1.0)),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    elif data == 'valid':\n",
    "        return A.Compose([\n",
    "            A.Resize(size, size),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfbc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(df=train_df, transform_fn=get_transforms(data=\"train\", size=Config.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f5f2c",
   "metadata": {},
   "source": [
    "## モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e301a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, config, out_dim, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = timm.create_model(self.config.model_name, pretrained=pretrained)\n",
    "        self.n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()  # custom head にするため\n",
    "        # nn.Identity() を使用することで、モデルの分類部（classifier）を無効化し、\n",
    "        # 代わりに self.fc という新しい分類層を追加して出力を行う\n",
    "        self.fc = nn.Linear(self.n_features, out_dim)\n",
    "\n",
    "    def feature(self, images):\n",
    "        feature = self.model(images)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, images, labels=None):\n",
    "        x = self.feature(images)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2df28a",
   "metadata": {},
   "source": [
    "## func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    config,\n",
    "    model,\n",
    "    dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    wandb_logger,\n",
    "    _custom_step,\n",
    "):\n",
    "#モデルを訓練モードに変更\n",
    "    model.train()\n",
    "    # PyTorchのAMP（Automatic Mixed Precision）を使用して、\n",
    "    # 浮動小数点演算の精度を調整しながら学習を行うためのスケーラーオブジェクトを作成します。\n",
    "    # これにより、モデルの訓練中に浮動小数点数のオーバーフローやアンダーフローを防ぎながら高速化できます。\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    losses = []\n",
    "    #len(dataloader)バッチの合計数,enumerateはindexとバッチに分かられる。\n",
    "    tbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    # batch = {\"images\": tensor1, \"targets\": tensor2}\n",
    "    \n",
    "    for step, batch in tbar:\n",
    "        # batchデータ\n",
    "        for k, v in batch.items():\n",
    "            #kはimageとtargetを含む\n",
    "            batch[k] = v.to(device)\n",
    "        targets = batch[\"targets\"]\n",
    "        batch_size = targets.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            batch_outputs = model(batch[\"images\"], labels=batch[\"targets\"])\n",
    "            loss = criterion(batch_outputs, targets)\n",
    "\n",
    "        if config.gradient_accumulation_steps > 1:\n",
    "            loss = loss / config.gradient_accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if config.clip_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad_norm)\n",
    "\n",
    "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            _custom_step += 1\n",
    "\n",
    "            if config.batch_scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "        wandb_logger.log(\n",
    "            {\"train_loss\": loss, \"lr\": scheduler.get_lr()[0], \"train_step\": _custom_step}\n",
    "        )\n",
    "\n",
    "        losses.append(float(loss))\n",
    "        tbar.set_description(f\"loss: {np.mean(losses):.4f} lr: {scheduler.get_lr()[0]:.6f}\")\n",
    "\n",
    "    loss = np.mean(losses)\n",
    "    return loss, _custom_step\n",
    "\n",
    "\n",
    "def valid_fn(\n",
    "    config,\n",
    "    model,\n",
    "    dataloader,\n",
    "    criterion,\n",
    "    device,\n",
    "    wandb_logger,\n",
    "    _custom_step,\n",
    "):\n",
    "    model.eval()\n",
    "    outputs, targets = [], []\n",
    "    losses = []\n",
    "\n",
    "    tbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, batch in tbar:\n",
    "        targets.append(batch[\"targets\"])\n",
    "\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "\n",
    "        batch_size = batch[\"targets\"].size(0)\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = model(batch[\"images\"], labels=batch[\"targets\"])\n",
    "            loss = criterion(batch_outputs, batch[\"targets\"])\n",
    "\n",
    "        if config.gradient_accumulation_steps > 1:\n",
    "            loss = loss / config.gradient_accumulation_steps\n",
    "\n",
    "        batch_outputs = torch.softmax(batch_outputs, dim=1)  # to proba\n",
    "        batch_outputs = batch_outputs.to(\"cpu\").numpy()\n",
    "        outputs.append(batch_outputs)\n",
    "\n",
    "        wandb_logger.log({\"valid_loss\": loss, \"valid_step\": _custom_step})\n",
    "        _custom_step += 1\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        tbar.set_description(f\"loss: {np.mean(losses):.4f}\")\n",
    "\n",
    "    outputs = np.concatenate(outputs)\n",
    "    targets = np.concatenate(targets)\n",
    "\n",
    "    loss = np.mean(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
